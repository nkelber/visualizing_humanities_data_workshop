{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Exploratory Data Analysis and Word Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to our third and final 😳notebook!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agenda today is the following:\n",
    "\n",
    "- Review homework assignment and answer questions\n",
    "- Intro to Exploratory Data Analysis \n",
    "- Discussion how we teach humanities data analysis and visualization\n",
    "- Discuss more complex visualizations (time-permitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's import our libraries and would anyone be willing to share their code and discuss their visualizations??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HOMEWORK REVIEW CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss Zoe's solution! I decided to stick with the humanist listserv and wanted to build from our initial word counts to see if I could uncover some more interesting patterns about early DH discourses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_vols = pd.read_csv('web_scraped_humanist_listserv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_vols['humanities_computing_counts'] = humanist_vols.text.apply(lambda x: x.count('humanities computing'))\n",
    "humanist_vols['digital_humanities_counts'] = humanist_vols.text.apply(lambda x: x.count('digital humanities'))\n",
    "humanist_vols.loc[:,'year'] = humanist_vols.dates.str.split('-').str[0]\n",
    "humanist_vols.loc[:,'year'] = pd.to_datetime(humanist_vols.year, format='%Y', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_melted = pd.melt(humanist_vols, id_vars=['dates', 'text', 'year'])\n",
    "humanist_melted = humanist_melted[['dates', 'variable', 'value', 'year']]\n",
    "\n",
    "humanist_melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's our initial chart from Wednesday\n",
    "alt.Chart(humanist_melted[humanist_melted.year.str.len() == 4]).mark_line().encode(\n",
    "    x='year:T',\n",
    "    y='value',\n",
    "    color='variable'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I honestly struggled to think of something more powerful that this line chart. I thought initially of counting more words but decided to try different chart styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(humanist_melted[humanist_melted.year.str.len() == 4]).mark_circle(\n",
    "    opacity=0.8,\n",
    "    stroke='black',\n",
    "    strokeWidth=1\n",
    ").encode(\n",
    "    alt.X('year:O', axis=alt.Axis(labelAngle=0)),\n",
    "    alt.Y('variable:N'),\n",
    "    alt.Size('value:Q',\n",
    "        scale=alt.Scale(range=[0, 400]),\n",
    "#         legend=alt.Legend(title='')\n",
    "    ),\n",
    "    alt.Color('variable:N', legend=None)\n",
    ").properties(\n",
    "    width=450,\n",
    "    height=200,\n",
    "    title='Comparison of Digital Humanities to Humanities Computing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bubble chart is a bit cleaner (thinking of the data-ink-ratio) but I think it's less compelling and more difficult to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(humanist_vols[humanist_vols.year.str.len() == 4][['digital_humanities_counts','humanities_computing_counts', 'year']]).mark_circle(size=100).encode(\n",
    "    x='digital_humanities_counts',\n",
    "    y='humanities_computing_counts',\n",
    "    color=alt.Color('year:O', legend=alt.Legend(columns=3, symbolLimit=0), scale=alt.Scale(scheme='plasma'))\n",
    ").properties(width=250)\n",
    "chart1 = alt.Chart(humanist_melted[humanist_melted.year.str.len() == 4]).mark_line().encode(\n",
    "    x='year:T',\n",
    "    y='value',\n",
    "    color='variable'\n",
    ").properties(width=250)\n",
    "(chart | chart1).resolve_scale(color='independent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the original chart with this scatterplot is helpful for seeing the relationship between the two but not sure it's much better than the line chart alone. The color scheme on the scatterplot is helpful but also not necessarily the most obvious to interpret either. Generally showing data with multiple plots is a good rule of thumb but not sure this is really telling us much more 🤔.\n",
    "\n",
    "I think part of the issue is that I'm not really sure besides this what might be of interest in this dataset. I could try exploring more terms and seeing their counts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['digital humanities', 'digital', 'humanities', 'humanities computing', 'computer']\n",
    "\n",
    "term_counts = []\n",
    "for term in terms:\n",
    "    humanist_terms = humanist_vols.copy()\n",
    "    humanist_terms[f'{term}_counts'] = humanist_terms.text.apply(lambda x: x.count(term))\n",
    "    humanist_terms['term'] = term\n",
    "    humanist_terms = humanist_terms[[f'{term}_counts', 'year', 'term']]\n",
    "\n",
    "    humanist_pivot = pd.pivot(humanist_terms, index='term', columns='year', values=f'{term}_counts').reset_index().rename_axis(None, axis=1)\n",
    "    term_counts.append(humanist_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remake our line chart\n",
    "humanist_concat = pd.concat(term_counts)\n",
    "humanist_melted = pd.melt(humanist_concat, id_vars=['term'])\n",
    "\n",
    "alt.Chart(humanist_melted[humanist_melted.variable.str.len() == 4]).mark_line().encode(\n",
    "    x='variable:T',\n",
    "    y='value',\n",
    "    color='term'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remake our scatterplot\n",
    "humanist_pivot = pd.pivot(humanist_melted, index='variable', columns='term', values='value').rename_axis(None, axis=1).reset_index() \n",
    "\n",
    "alt.Chart(humanist_pivot[humanist_pivot.variable.str.len() == 4]).mark_circle().encode(\n",
    "    alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    "    color=alt.Color('variable:T', legend=alt.Legend(columns=3, symbolLimit=0), scale=alt.Scale(scheme='plasma'))\n",
    ").properties(\n",
    "    width=150,\n",
    "    height=150\n",
    ").repeat(\n",
    "    row=terms,\n",
    "    column=terms\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we are starting to see some interesting patterns. Though we knew that digital humanities and humanities computing did not really correlate, it seems like digital, computer, and humanities computing. We also see that humanities actually does correlate with both humanities computing and digital. However, only one of these is likely signal given how Python does string matching.\n",
    "\n",
    "What else could we do with counting to explore more trends in this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two very popular approaches is using Term Frequency Inverse Document Frequency (which you can read more about here https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf) or Named Entity Recognition. \n",
    "\n",
    "Let's try them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I pre-processed this dataset for NER and you can look at the code in the identify_ner.py script. The reason this is a script and not a note book is because it is VERY slow!\n",
    "humanist_vols = pd.read_csv('smaller_humanist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_vols.loc[:, 'year'] = humanist_vols.dates.str.split('-').str[0]\n",
    "humanist_vols.loc[:,'datetime'] = pd.to_datetime(humanist_vols.year.astype(str) + '-' + humanist_vols.month.astype(str) + '-01', format='%Y-%m-%d', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_vols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_vols[humanist_vols.year.str.len() ==4][['datetime', 'ner_terms', 'cleaned_terms']].to_csv('smaller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_vols.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try TF-IDF. This code is from the PH Tutorial linked above\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "all_docs = humanist_vols.cleaned_terms.tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=.7, min_df=1, stop_words=None, use_idf=True, norm=None)\n",
    "transformed_documents = vectorizer.fit_transform(all_docs)\n",
    "\n",
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "all_files = humanist_vols.datetime.astype(str).tolist()\n",
    "tfidf_results = []\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # construct a dataframe\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "    one_doc_as_df['datetime'] = all_files[counter]\n",
    "    tfidf_results.append(one_doc_as_df)\n",
    "#     print(one_doc_as_df[0:1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tfidf_refresh](https://miro.medium.com/max/1838/1*qQgnyPLDIkUmeZKN2_ZWbQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.concat(tfidf_results)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df.sort_values(by=['score'], ascending=False)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the most unique across the corpus terms\n",
    "alt.Chart(tfidf_df[0:10]).mark_bar().encode(\n",
    "    x='score:Q',\n",
    "    y=alt.Y('term:N', sort='-x'),\n",
    "    color='datetime:N'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also split it by email chain\n",
    "tfidf_grouped = tfidf_df.groupby('datetime').apply(lambda x: x.nlargest(5, 'score')).reset_index(drop=True) \n",
    "tfidf_grouped\n",
    "\n",
    "alt.Chart(tfidf_grouped[0:100]).mark_bar().encode(\n",
    "    x='score:Q',\n",
    "    y=alt.Y('term:N', sort='-x'),\n",
    "    color='datetime:N',\n",
    ").facet(\n",
    "    facet='datetime:N',\n",
    "    columns=3\n",
    ").resolve_scale(y='independent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try out NER\n",
    "humanist_vols.loc[:,'ner_tokens'] = humanist_vols.apply(lambda row: nltk.word_tokenize(row['ner_terms']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for more info checkout https://spacy.io/usage/linguistic-features#named-entities\n",
    "![ner](https://miro.medium.com/max/1400/0*zDbB-LV-Dlm_F_PX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_exploded = humanist_vols.explode('ner_tokens')\n",
    "humanist_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_grouped = humanist_exploded.groupby(['datetime', 'ner_tokens']).size().reset_index(name='frequency')\n",
    "humanist_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanist_grouped.sort_values(by=['datetime', 'frequency' ], inplace=True, ascending=False)\n",
    "humanist_top = humanist_grouped.sort_values(by=['datetime','frequency'],ascending = [False, False]).groupby(['datetime']).head(5).sort_index()\n",
    "humanist_pivot = humanist_top.pivot(index='ner_tokens', columns='datetime', values='frequency').fillna(0)\n",
    "humanist_top_terms = humanist_pivot.unstack().reset_index(name='frequency')\n",
    "humanist_top_terms.sort_values(by=['frequency'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = humanist_top_terms[0:70].ner_tokens.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(humanist_top_terms[humanist_top_terms.ner_tokens.isin(top_terms)][['datetime', 'ner_tokens', 'frequency']]).mark_line().encode(\n",
    "    x = alt.X('datetime:T', axis=alt.Axis(labelAngle=0)),\n",
    "    y = alt.Y('frequency:Q'),\n",
    "    color = alt.Color('ner_tokens:N', legend=alt.Legend(title='Place Name Mentioned on Humanist Listserv')),\n",
    "    tooltip = [alt.Tooltip('ner_tokens', title='Place Identified'), 'frequency']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = alt.selection_multi(fields=['ner_tokens'], bind='legend')\n",
    "alt.Chart(humanist_top_terms[humanist_top_terms.ner_tokens.isin(top_terms)][['datetime', 'ner_tokens', 'frequency']]).mark_circle(\n",
    "    opacity=0.8,\n",
    "    stroke='black',\n",
    "    strokeWidth=1\n",
    ").encode(\n",
    "    x = alt.X('datetime:T', axis=alt.Axis(labelAngle=0)),\n",
    "    y = alt.Y('ner_tokens:N'),\n",
    "    size= alt.Size('frequency:Q',\n",
    "        scale=alt.Scale(range=[0, 400]),\n",
    "    ),\n",
    "    color = alt.Color('ner_tokens:N', legend=alt.Legend(title='Place Name Mentioned on Humanist Listserv')),\n",
    "    opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
    "    tooltip = [alt.Tooltip('ner_tokens', title='Place Identified'), 'frequency']\n",
    ").add_selection(\n",
    "    selection\n",
    ").properties(\n",
    "    width=450,\n",
    "    height=320,\n",
    "    title=''\n",
    ").configure_legend(labelLimit= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 30\n",
    "overlap = 1\n",
    "alt.Chart(humanist_top_terms[humanist_top_terms.ner_tokens.isin(top_terms)], height=step).mark_line().encode(\n",
    "    y=alt.Y('frequency:Q', scale=alt.Scale(range=[step, -step * overlap]), axis=None),\n",
    "    x='datetime:T',\n",
    "    color=alt.Color('ner_tokens:N', legend=None),\n",
    ").facet(\n",
    "    row=alt.Row(\n",
    "        'ner_tokens:N',\n",
    "        title=None,\n",
    "        header=alt.Header(labelAngle=0, labelAlign='right'),\n",
    "    )\n",
    ").properties(\n",
    "    bounds='flush',\n",
    "    title='Frequency of Top Place Names Appearing on the Humanist Listserv, 1987-2007'\n",
    ").configure_facet(\n",
    "    spacing=0\n",
    ").configure_view(\n",
    "    stroke=None\n",
    ").configure_title(\n",
    "    anchor='middle'\n",
    ").configure_axisY(\n",
    "    labelPadding=-10, \n",
    "    labelAlign='right'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with Humanist Listserv Data\n",
    "\n",
    "**Need to contextualize our data**\n",
    "\n",
    "\n",
    "Example 1: Julianne Nyhan \"In Search of Identities in the Digital Humanities: The Early History of Humanist\" [https://www.researchgate.net/publication/280836611_In_Search_of_Identities_in_the_Digital_Humanities_The_Early_History_of_Humanist](https://www.researchgate.net/publication/280836611_In_Search_of_Identities_in_the_Digital_Humanities_The_Early_History_of_Humanist)\n",
    "\n",
    "\n",
    "- Gives the history of the listserv, overview of the posts, and argues about the creation of disciplinary identity\n",
    "- \"Terms used to describe the group seem to signal how idealistic and personally involved a number of the early practitioners were. These included Suppor[t]ers of computing in the Humanities (Humanist 1:44); free people (1:80); true believer[s] (1:1035) and the lament “I thought we were all in this together” (1:661)\"\n",
    "\n",
    "---\n",
    "\n",
    "Example 2: Rockwell, Geoffrey, and Stéfan Sinclair. “The Swallow Flies Swiftly Through: An Analysis of\n",
    "Humanist.” [http://hermeneuti.ca/swallow-flies](http://hermeneuti.ca/swallow-flies)\n",
    "\n",
    "- Does some text analysis of the listserv\n",
    "- Compares DH to humanities computing, rise of the web, rise of software and social media discourses\n",
    "- Makes an argument about hack vs yack\n",
    "\n",
    "---\n",
    "\n",
    "Example 3: David McClure’s “Visualizing 27 years, 12 million words of the Humanist list” [http://humanist.dclure.org/](http://humanist.dclure.org/) and [http://dclure.org/essays/visualizing-the-humanist/](http://dclure.org/essays/visualizing-the-humanist/)\n",
    "\n",
    "\n",
    "- Using kde to compute patterns in text and compare the similarity in patterns. Finally use those similarities as edges in a network.\n",
    "- 1980s computer hardware and textual studies\n",
    "- 1990s increase in place names\n",
    "- 2000s rise of administrative language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
